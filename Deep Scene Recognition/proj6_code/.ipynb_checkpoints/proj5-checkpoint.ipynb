{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fiK4QNWKDvWJ",
    "outputId": "3472566e-5e30-47b8-a3f4-b67bf3a89af0"
   },
   "source": [
    "# Scene Recognition with Deep Learning\n",
    "For this project we are going to focus for image classification on the 15-scene data with the state-of-the-art approach: deep learning.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
    "2. Understand the concepts behind different layers, optimizers, and learning schedules;\n",
    "3. Experiment with different models and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter\n",
    "code does not crash when run unmodified and you can get a preview of how\n",
    "results are presented.\n",
    "\n",
    "This project is intended to further familiarize you with Python, PyTorch. Once again, you may find these resources helpful. Python: [here](https://docs.python.org/3/tutorial/). PyTorch: [here](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief\n",
    "    Hand-in: through Canvas\n",
    "    Required files: <your_uid>.zip.(Please begin with 'u' for your uid)\n",
    "                    <your_uid>_proj5.pdf\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "Take note that some of the concepts used in this project have NOT been covered in lectures, hence you may want to use this instruction page as the reference material when you proceed to each section.\n",
    "\n",
    "We will be installing a NEW environment for this project; follow the instructions below to set up the env. If you run into import module errors, try “pip install -e .” again, and if that still doesn’t work, you may have to create a fresh environment.\n",
    "\n",
    "Note that although we are training a neural net from scratch for this project, your laptop should be sufficient to handle this (expecting a 5 to 10 minutes training time for Part 1 and 2, and roughly 20 to 30 minutes for Part 3 with only the CPU); you are free to use Google Colab on this, but you may need to figure out a way of putting both the notebook and the dataset into your Google Drive and mount it in the Colab notebook (this tutorial covers everything you need to know to set it up).\n",
    "    \n",
    "   0. Unzip proj5_6320.zip and go to proj5_6320 directory.\n",
    "      - You can run `unzip proj5_6320.zip && mv proj5_train.zip proj5_6320/data && mv proj5_test.zip proj5_6320/data` in your terminal.\n",
    "      - Run `cd proj5_6320/data && unzip proj5_train.zip && unzip proj5_test.zip && cd ..`\n",
    "   1. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html). It doesn’t matter whether you use Python 2 or 3 because we will create our own environment that uses 3 anyways.\n",
    "   2. Create a conda environment using the appropriate command. On Windows, open the installed “Conda prompt” to run the command. On MacOS and Linux, you can just use a terminal window to run the command, Modify the command based on your OS (linux, mac, or win): `conda env create -f proj5_env_<OS>.yml`.\n",
    "    - NOTE that `proj5_env_<OS>.yml` is inside the project folder.\n",
    "   3. This should create an environment named ‘proj5’. Activate it using the Windows command, activate proj5 or the MacOS / Linux command, `source activate proj5`\n",
    "   4. Install the project package, by running `pip install -e .` inside the repo folder.\n",
    "   5. Run the notebook using `jupyter notebook` under *proj5_6320* directory.\n",
    "   6. Ensure that all sanity checks are passing by running pytest tests inside the \"proj5_unit_tests/\" folder.\n",
    "   7. Generate the zip folder for the code portion of your submission once you’ve finished the project using \n",
    "    \n",
    "        `python zip_submission.py --uid <your_uid>` \n",
    "\n",
    "\n",
    "<!-- ## Library Functions \n",
    "Do not use any library functions to implement Hough Transform. -->\n",
    "\n",
    "\n",
    "### Writeup\n",
    "For this project, you need to run all your cells and then convert the notebook into pdf. A conversion way is to use the tool provided by Jupyter. Click the items on the menu of this website page:\n",
    "\n",
    "`File -> Download as -> PDF via LaTeX(.pdf)`\n",
    "    \n",
    "You can download it as HTML if the above option doesn't work.\n",
    "    \n",
    "You can refer [here](1) in case you encounter any problem. \n",
    "    \n",
    "[1]: https://stackoverflow.com/questions/29156653/ipython-jupyter-problems-saving-notebook-as-pdf\n",
    "    \n",
    "Your code, results, visualization, and discussion will be used for the grading. You will be deducted points if the results are not shown in this notebook. Do not change the order of the cells. You can add cells in need. You can copy a cell and run it seperately if you need to run a cell multiple times and thus every result is displayed in the cell.\n",
    "    \n",
    "### Submission Format\n",
    "\n",
    "This is very important as you will lose 5 points for every time you do not follow the instructions. You will attach two items in your submission on Canvas:\n",
    "\n",
    "1. `<your_uid>`.zip containing:\n",
    "    `<your_uid>`.zip via Canvas containing:\n",
    "    - proj5_code/ - directory containing all your code for this assignment\n",
    "    - README.txt - (optional) if you implement any new functions other than the ones we define in the skeleton code (e.g. any extra credit implementations), please describe what you did and how we can run the code. We will not award any extra credit if we can’t run your code and verify the results.\n",
    "\n",
    "2. `<your_gt_username>`_proj5.pdf - your report\n",
    "\n",
    "Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that’s not in there by default will probably cause your code to break during grading. Do not use absolute paths in your code or your code will break. Use relative paths like the starter code already does. Failure to follow any of these instructions will lead to point deductions. Create the zip file using python zip_submission.py --uid `<your_uid>` (it will zip up the appropriate directories/files for you!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric\n",
    "* +25 pts: Part1   \n",
    "* +30 pts: Part2\n",
    "* +35 pts: Part3\n",
    "* +10  pts: pdf report\n",
    "\n",
    "Distribution of the points in a Question is separately mentioned for each sub-task\n",
    "\n",
    "* -5*n pts: Lose 5 points for every time you do not follow the instructions for the hand-in format.\n",
    "    \n",
    "### Extra Credits\n",
    "    \n",
    "SimpleNet:\n",
    "\n",
    "* Up to 4 pts: In Part 2, modify the network architecture and achieve a final testing accuracy of 63%; note that you are NOT allowed to add more nn.Conv2d into the model; instead, think about how to improve the model with a concept called “batch normalization”.\n",
    "\n",
    "AlexNet and beyond:\n",
    "\n",
    "* Up to 4 pts: Fine-tune your AlexNet and reach a testing accuracy of 93%.\n",
    "* Up to 6 pts: Fine-tune another network like torchvision.models.resnet, and reach a testing accuracy of 90%.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "The dataset to be used in this assignment is the 15-scene dataset, containing natural images in 15 possible scenarios like bedrooms and coasts. It’s first introduced by [Lazebnik et al, 2006](https://www.di.ens.fr/willow/pdfs/cvpr06b.pdf). The images have a typical size of around 200 by 200 pixels, and serve as a good milestone for many vision tasks. A sample collection of the images can be found below:\n",
    "<img src=\"figures/dataset.png\" alt=\"drawing\" width=\"800\" title=\"eval\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "g1dqr6qSBpE2",
    "outputId": "2a6bb055-8fb2-4b0e-b137-31efc09c375b"
   },
   "outputs": [],
   "source": [
    "from proj5_code.runner import Trainer\n",
    "from proj5_code.optimizer import get_optimizer\n",
    "from proj5_code.simple_net import SimpleNet, SimpleNet2\n",
    "from proj5_code.simple_net_dropout import SimpleNetDropout\n",
    "from proj5_code.my_alexnet import MyAlexNet\n",
    "from proj5_code.image_loader import ImageLoader\n",
    "from proj5_code.data_transforms import get_fundamental_transforms, get_data_augmentation_transforms\n",
    "from proj5_code.stats_helper import compute_mean_and_std\n",
    "\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj5_unit_tests.test_base import verify\n",
    "from proj5_unit_tests.test_stats_helper import test_mean_and_variance\n",
    "from proj5_unit_tests.test_image_loader import test_dataset_length, test_unique_vals, test_class_values, test_load_img_from_path\n",
    "from proj5_unit_tests.test_data_transforms import test_fundamental_transforms\n",
    "from proj5_unit_tests.test_dl_utils import test_predict_labels, test_compute_loss\n",
    "from proj5_unit_tests.test_simple_net import test_simple_net\n",
    "from proj5_unit_tests.test_simple_net_dropout import test_simple_net_dropout\n",
    "from proj5_unit_tests.test_my_alexnet import test_my_alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjE0jIc5BpFN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On this machine, there is no cuda\n"
     ]
    }
   ],
   "source": [
    "is_cuda = True\n",
    "is_cuda = is_cuda and torch.cuda.is_available() # will turn off cuda if the machine doesnt have a GPU\n",
    "\n",
    "if is_cuda:\n",
    "    print(\"On this machine, there is cuda\")\n",
    "else:\n",
    "    print(\"On this machine, there is no cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: A SimpleNet: Dataset, Model, and Trainer\n",
    "\n",
    "**Learning Objective:** \n",
    "\n",
    "    (1) Understanding the rationale behind data pre-processing, \n",
    "    (2) construct a basic Convolutional Neural Net for multi-class classification, \n",
    "    (3) understand the use of some basic layers used in neural net, and \n",
    "    (4) set up the training workflow in PyTorch.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this part, we are going to train a simple convolutional neural net from scratch. We’ll be starting with some modification to the dataloader used in this project to include a few extra pre-processing steps. Subsequently, you will define your own model and optimization function. A trainer class will be provided to you, and you will be able to test out the performance of your model with this complete pipeline of classification problem.\n",
    "\n",
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value.\n",
    "\n",
    "### Experiment and Report:\n",
    "For Part 1, note that you are required to get a final validation accuracy of **50%** to receive full credits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGSv2QfBBpFZ"
   },
   "source": [
    "### Part 1.1: Datasets, Dataloader, and Data Transforms\n",
    "In this part you’ll be implementing the basic Datasets object, which helps to retrieve the data from your local data folder, and prepare to be used by your model. To start with, finish the compute_mean_and_std() function in stats_helper.py and compute the mean and standard deviation of the dataset. Check out the relevant function in [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which might be helpful for this; you may probably want to use the function partial_fit here. **Note** that here we want to compute the mean and standard deviation for **all pixels across all images** after you have normalized each image to the range [0, 1], hence when you apply partial_fit, your feature dimension should be 1 and the returning mean and standard deviation should both be a single value. Think about how you can achieve this by reshaping the input to the scaler.\n",
    "\n",
    "Next, fill in the ImageLoader class in image_loader.py. Note that similar to what you have implemented in Project 1, here the ImageLoader class contains the paths to the dataset images, and should be able to return the expected element given an index. More details can be found in the file.\n",
    "\n",
    "Additionally, in data_transforms.py, complete the function get_fundamental_transforms(): resize the input, convert it to a tensor, and normalize it using the passed in mean and standard deviation. You may find torchvision.transforms.Resize, torchvision.transforms.ToTensor, torchvision.transforms.Normalize, and torchvision.transforms.Compose useful here, and it should only take a few lines of code.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "One crucial aspect of deep learning is to perform data preprocessing. In this project we are going to \"zero-center\" and \"normalize\" the dataset. \n",
    "\n",
    "### Compute mean and standard deviation of the dataset\n",
    "To begin with, fill in the `compute_mean_and_std()` in `stats_helper.py` to compute the **mean** and **standard deviation** of both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWA_2UbjBpFd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your mean and std computation:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your mean and std computation: \", verify(test_mean_and_variance))\n",
    "dataset_mean, dataset_std = compute_mean_and_std('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xixFr8CDBpFn",
    "outputId": "267d219f-8089-4b25-9e4d-c9cc5a8699e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mean = [0.45547486], standard deviation = [0.25316328]\n"
     ]
    }
   ],
   "source": [
    "print('Dataset mean = {}, standard deviation = {}'.format(dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2TeGbrQBpFu"
   },
   "source": [
    "Now let's create the **Datasets** object to be used later. Remember back in Project 1, we have initialized such a class to load 5 images? Here the task is similar: download the data into the project folder, and then complete the `image_loader.py`. The essence is to retrieve the paths to all the images required, and be able to provide the **path** and the **class id** when given an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THRvAvluXFcS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your image loader (length): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (values): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (classes): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (paths): \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inp_size = (64,64)\n",
    "print(\"Testing your image loader (length):\", verify(test_dataset_length))\n",
    "print(\"Testing your image loader (values):\", verify(test_unique_vals))\n",
    "print(\"Testing your image loader (classes):\", verify(test_class_values))\n",
    "print(\"Testing your image loader (paths):\", verify(test_load_img_from_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transforms\n",
    "For this part, complete the function `get_fundamental_transforms()` in `data_transforms.py` to compile a list of fundamental transforms which:\n",
    "1. Resize the input image to the desired shape;\n",
    "2. Convert it to a tensor;\n",
    "3. Normalize them based on the computed mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your fundamental data transforms:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: SimpleNet:Model\n",
    "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. Now open the simple_net.py and fill in the model definition. By now you should have a decent grasp on how to properly define a deep learning model using nn.Conv2d, nn.ReLU, nn.MaxPool2d, etc. For this part, define a convolutional neural network with 2 conv layers (and the corresponding ReLU, MaxPool, and Fully Connected layers) which aligns with our training dataset (15 classes). Below you may find a sample network architecture which is sufficient to get you pass Part 1 (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1).\n",
    "<img src=\"figures/net.jpg\" alt=\"drawing\" width=\"1000\" title=\"eval\"/>\n",
    "\n",
    "After you have defined the proper model, now fill in the forward function, which accepts the input (a batch of images), and generates the output classes (this should only take a couple of lines of code).\n",
    "\n",
    "### Model\n",
    "To get you started in this part, simply define a **2-layer** model in the `simple_net.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters to make the tensor flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvVL-ap0BpFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your SimpleNet architecture:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleNet2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5d4f2ee68f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimple_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimple_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleNet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleNet2' is not defined"
     ]
    }
   ],
   "source": [
    "simple_model = SimpleNet()\n",
    "simple_model = SimpleNet2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "When defining your model architecture, also initialize the `loss_criterion` variable there. Remeber this is multi-class classification problem, and think about what loss function might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=15, bias=True)\n",
      "  )\n",
      "  (loss_criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Next, **initialize the following cell with proper values for learning rate and weight decay** (you can come back and tune these values for better performance once the trainer section is done), and then fill in the `optimizer.py` to initialize a basic optimization function; this should only take a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2cwtK5PBpF7"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 0.001,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"momentum\": 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0CrYZa4BpGE"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model, optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: SimpleNet: Trainer\n",
    "We have provided you with a basic trainer (check out the runner.py) where we’ll load in the dataset and train the network you have defined for some epochs. To complete this section, first assign proper values to the dict optimizer_config in the Jupyter notebook with reasonable **learning rate** and **weight decay**, and then fill in the optimizer.py with the default optimization function Adam.\n",
    "\n",
    "Next, in dl_utils.py, complete predict_labels() and compute_loss(), where you are given an input model, and calculate the corresponding predictions and loss respectively; these functions will be used in the training process later.\n",
    "\n",
    "Lastly, train it using the starter code in Jupyter Notebook, and report the training process according to the report. Take note that you may use these values (“lr” and “weight_decay”) as a starting point to tune your parameters to pass this part.\n",
    "\n",
    "You should be able to achieve a 50% accuracy with ease.Here a couple of lines of code for a 2 layer neural net have easily defeated that!\n",
    "\n",
    "Note that although you are **NOT** required to modify anything in the runner.py, it is still highly recommended to read through the code and understand how to properly define the training process, as well as record the loss history.\n",
    "\n",
    "### Trainer\n",
    "Next we define the trainer for the model; to start, complete the `predict_labels()` and `compute_loss()` in `dl_utils.py`: given a model, compute the corresponding predictions and loss respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your trainer (model prediction):  \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your trainer (loss values):  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your trainer (model prediction): \", verify(test_predict_labels))\n",
    "print(\"Testing your trainer (loss values): \", verify(test_compute_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass in the model, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
    "\n",
    "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **30** is good enough to get you around the threshold for Part 1, and you are free to increase it a bit and adjust other parameters in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiGOvPJfBpGO"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = simple_model,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = '../model_checkpoints/simple_net',\n",
    "                  train_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "paNLyU5cBpGX",
    "outputId": "56af8728-d91c-4886-c73b-d12044ffe40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:1.8662\n",
      "Epoch:2, Loss:1.8372\n",
      "Epoch:3, Loss:1.3911\n",
      "Epoch:4, Loss:2.0194\n",
      "Epoch:5, Loss:2.3451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d6ea218ac3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sandbox/cs6320-project-solutions/proj5_6320/proj5_code/runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_accuracy_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_accuracy_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sandbox/cs6320-project-solutions/proj5_6320/proj5_code/runner.py\u001b[0m in \u001b[0;36meval_on_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         input_data, target_data = Variable(\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sandbox/cs6320-project-solutions/proj5_6320/proj5_code/image_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Apply the transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proj5/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and testing data. Copy the accuracy plots and values onto the report, and answer the questions there. Note that you are required to obtain a **50%** testing accuracy to receive full credits for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z0b_WwJhBpGf",
    "outputId": "8d299888-33c6-4234-dc78-5585c9888057",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8epn0IBmBpGn",
    "outputId": "a9e4d15d-9ac9-4b38-cf26-86a93f734495",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.0; Validation Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7hE1vidBpGt"
   },
   "source": [
    "## Part 2: Overfitting\n",
    "Feeling good? We have easily obtained a 50% accuracy on the testing data with a very simple model. Feeling even better for the training accuracy right? More than 90% (if you have implemented everything correctly). But should you?\n",
    "\n",
    "Our final accuracies for training and testing data differ a lot from each other, which indicates that the model we have defined **fits too well with the training data, but is unable to generalize well on data it has never seen before**: this is often regarded as **overfitting**. In this section we are going to apply 2 techniques to tackle with it: adjusting both data and model.\n",
    "\n",
    "**Learning Objective:**\n",
    "\n",
    "    (1) Understanding the effects and cause of **overfitting**, \n",
    "    (2) learn about the basic techniques to avoid overfitting, \n",
    "    (3) understand the common data augmentation and transforms used in PyTorch, and \n",
    "    (4) apply the concept of **dropout** in the network.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "If you have been examining the performance from the previous section, like loss and accuracy, you will notice that our training accuracy is somewhere around **98~100%**, if you have implemented everything correctly. By contrast, the threshold for passing Part 1 is a **50%** accuracy of the testing data; this difference is often regarded as “overfitting”: the model performs much better on the training data, but is not able to generalize well on data it has never seen previously. Normally, 2 reasons lead to this effect:\n",
    "\n",
    "   * **Data:** If you have examined the dataset, you’ll notice that the number of images used for training totals at around 3000; this could be a relatively small number considering we have 15 classes together; can we do better? There are some real-life scenarios where we could get some insights: if you left-right flip (mirror) an image of a scene, it never changes categories. A kitchen doesn’t become a forest when mirrored. This isn’t true in all domains – a “d” becomes a “b” when mirrored, so you can’t “jitter” digit recognition training data in the same way. But we CAN synthetically increase our amount of training data by left-right mirroring training images during the learning process in this particular project.\n",
    "   \n",
    "   * **Model:** We have trained our model to fit well with the training data, but obviously it’s doing “too well”. To counter this, we are going to use a technique called “dropout” to regularize the network. What does dropout regularization do? It randomly turns off network connections at training time to fight overfitting. This prevents a unit in one layer from relying too strongly on a single unit in the previous layer. Dropout regularization can be interpreted as simultaneously training many “thinned” versions of your network. At test time all connections are restored, which is analogous to taking an average prediction over all of the “thinned” networks.\n",
    "   \n",
    "### Experiment and Report:\n",
    "\n",
    "For Part 2, note that you are required to get a final testing accuracy of **55%** to receive full credits. Similar to Part 1, you may need to start with some optimization configuration values in the Jupyter notebook, and gradually tune it for better performance. Also, take a screenshot of what you have done in get_data_augmentation_transforms(), and put it inside the report.\n",
    "\n",
    "### Part 2.1: Jitter, Random Flip, and Normalization\n",
    "One common technique to increase the \"variability\" of the data is to **augment** it. Firstly, we don't have a huge amount of data, so let's \"jitter\" based on it; secondly, when you mirror an image of a **kitchen**, you can tell that the mirrored image is still a kitchen. \n",
    "\n",
    "Open the data_transforms.py: you’ll find some starter code where the data input is cropped in the center and then transformed into a tensor. Add another 2 data transformations in `get_data_augmentation_transforms()` function in `data_transforms.py` where you could jitter the data, and randomly flip the data left-to-right. You may find the methods transforms.RandomHorizontalFlip() and transforms.ColorJitter() helpful in this part. Pay attention to the **position** where you insert those transforms! You may first copy your existing fundamental transform implementation into this function, and then insert a couple of other transforms which help you do the above adjustment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ech4Y22OXOui"
   },
   "outputs": [],
   "source": [
    "inp_size = (64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Dropout\n",
    "\"Dropout\" is a technique commonly used to regularize the network. It randomly turns off the connection between neurons inside the network and prevent the network from relying too much on a specific neuron. Follow the below instruction and finish the `simple_net_dropout.py` with your previous SimpleNet model, plus the dropout layer, and lastly re-run the training process as below.\n",
    "\n",
    "Open the simple_net_dropout.py, and first copy your existing “SimpleNet” model into this file, and then append a dropout layer in the model. You may use the default dropout probability from nn.Dropout() for regularization.\n",
    "\n",
    "With the new data augmentation and dropout added into the model, let’s re-run the trainer and check its performance. Follow the template code used in the notebook, and train the “regularized” network with the new data transforms. You should be seeing that compared with Part 1, your training accuracy drops but testing accuracy improves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNetDropout architecture: \", verify(test_simple_net_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlgm5eM-BpGz"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "print(simple_model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous part, **initialize the following cell with proper values for learning rate and weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btKIvIrdBpG5"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 1e-10,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peqS_C6QBpG_"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model_dropout, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExoLylurBpHH"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = simple_model_dropout,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = '../model_checkpoints/simple_net_dropout',\n",
    "                  train_data_transforms = get_data_augmentation_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will take longer than Part 1, as now we have more data (and more variability), and the model is slightly more complicated than before as well; however, it should finish within 10~15 minutes anyway, and the default `num_epochs` is also good enough as a starting point for you to pass this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "id": "-ljUl4UnBpHN",
    "outputId": "c3542fae-c2dc-495b-dd49-d5bb85c7fd79",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous part, now plot out the loss and accuracy history. You'll need to pass a threshold of **55%** to receive full credits for this part. Also copy the plots onto the report, and answer the questions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Gdh9AvHIBpHW",
    "outputId": "4fd9e6eb-ebcf-4bbb-a9e6-79db49777661",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6SLuc3zmBpHd",
    "outputId": "4069ff19-8bcc-44b3-9cd5-8bc8a63e626d"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amh1wxlJBpHj"
   },
   "source": [
    "## Part 3: AlexNet\n",
    "You can see that after the above adjustment, our model performance increases in terms of testing accuracy. Although the training accuracy drops, now it's closer to the testing values and that's more natural in terms of performance. But we are not satisfied with the final performance yet. Our model, in the end, is still a 2-layer SimpleNet and it might be capable of capturing some features, but could be improved a lot if we go **deeper**. In this part we are going to see the power of a famous model: AlexNet.\n",
    "\n",
    "**Learning Objective:** \n",
    "\n",
    "    (1) Understanding the rationale of deep learning, \n",
    "    (2) learn to utilize pretrained models defined by PyTorch, \n",
    "    (3) understand the concept of fine-tuning, and \n",
    "    (4) achieve good classification results with the help of pretrained model weights.\n",
    "    \n",
    "### Introduction\n",
    "\n",
    "Ever since 2012, deep learning has been extremely successful in vision related tasks, and everything blows up from a model named AlexNet.\n",
    "\n",
    "Training a relatively complex model like AlexNet completely from scratch is sometimes overwhelming, especially when GPU is not available. However, PyTorch has already incorporated the pretrained weights of some famous model and provide you with the option of loading it directly.\n",
    "\n",
    "#### Part 3.1: AlexNet Model Definition\n",
    "Now switch to my_alexnet.py, and first load the AlexNet using model=torchvision.models.alexnet(pretrained=True).\n",
    "\n",
    "Note that in the original AlexNet model, there are **1000** output classes, while we have 15 for this project. Here we are going to retrieve some layers from an existing network, and concatenate them with your own custom layers: in simple_net.py, when you have obtained the AlexNet model, retrieve the convolutional layers (which helps you to detect the features) and the fully connected layers (which generates the classification scores), remove the last Linear layer, and replace it with a proper layer which could output the scores for 15 classes. You may find module.children() helpful here.\n",
    "\n",
    "#### Part 3.2: Fine-tuning AlexNet\n",
    "Notice that what we have done is merely defining the final layer with the correct dimensions, but the weights of the layer are just some random values and it won’t be able to produce the desired scores. In the Jupyter Notebook, follow the starter code and train the newly defined network. Note that AlexNet has millions of parameters, so training it from scratch could cause problems for your laptop with no GPU support, hence here we are only training it for 5 epochs and the expected running time is around 20 minutes without a GPU. In addition, note that you do NOT need to update the weights for earlier layers of the model, therefore, freeze the weights of the convolution layers and some of the fully connected layers of the model (think about which FC layers to freeze by printing out the model architecture), and then train the network following the template code in the notebook, observe the performance and answer the questions in the report. You may find weight.requires_grad and bias.requires_grad helpful here.\n",
    "\n",
    "### Experiment and Report:\n",
    "\n",
    "For Part 3, note that you are required to get a final testing accuracy of 85% to receive full credits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttbhBZ7CXUng"
   },
   "outputs": [],
   "source": [
    "inp_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 & 3.2: Fine-tuning the AlexNet\n",
    "Now switch to `my_alexnet.py`, and define a AlexNet which can be fit onto our dataset: PyTorch has provided us with pre-trained models like AlexNet, so what you want to do is to load the model first, and then adjust some of the layers such that it fits with our own dataset, instead of outputing scores to 1000 classes from the original AlexNet model.\n",
    "\n",
    "After you have defined the correct architecture of the model, make some tweaks to the existing layers: **freeze** the **convolutional** layers and first 2 **linear** layers so we don't update the weights of them; more details can be found in the instruction webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your AlexNet architecture: \", verify(test_my_alexnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBoLgRrlBpHl"
   },
   "outputs": [],
   "source": [
    "my_alexnet = MyAlexNet()\n",
    "print(my_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6AYkHAgBpHw"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 1e-10,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72N8uwsZBpIA"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(my_alexnet, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtCIaTMmBpIK"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = my_alexnet,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = '../model_checkpoints/myalexnet/',\n",
    "                  train_data_transforms = get_data_augmentation_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training cell will take roughly 20 minutes or slightly more using CPU (but possibly under 5 minute using GPU depending on the batch size; the TAs got it within 3 minutes on a GTX1060)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "CAcncwLPBpIQ",
    "outputId": "7a341877-2628-4acf-e99a-a1c96ff51fd8"
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like both previous sections, you are required to pass a threshold of **85%** for this part. Copy the plots and values onto the report and answer questions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Cimj95G_BpIU",
    "outputId": "f2356e0d-b2c4-4a13-ff63-4516ee5193d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qMI3CdEuBpIb",
    "outputId": "ffccde71-d62e-4eae-cd2e-241847b9ee23"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these we have concluded the last project of CS6320 Computer Vision. Things might be hard along the way, but we hope you enjoyed this journey and have learnt something in this field. Our team has learnt a lot from you guys as well, so thank you and wish you all the best in your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dl.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
